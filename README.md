# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**This dataset contains data about bank marketing. The point of the dataset tells whether an person ends up subscribing to a term deposit. Using this data to create a classification model, we can predict whether a person is likely to subscribe to a term deposit.**

**We tested two ways to create classification models using AzureML. The first was using linear regression with parameters tuned by Hyperdrive. The second was using AutoML which automated model creation and parameter selection. The best preforming model was an AutoML Voting Ensamble with an accuracy of 91.8%. The Hyperdrive logistic regression model scored close with a 91.3%.**

## Scikit-learn Pipeline
**The Scikit-learn pipeline used was a logistic regression model. This combined a train Python file which describes how the data is prepared and cleaned as well as the model creation. A jupyter notebook was then used add the Hyperdrive component which ran multiple experiments using different hyperparameters. **

**I used a random parameter sampling with regularization chosen from a uniform distribution between 0 and 1 and max iterations chosen from the values 10, 40, 80, 100, 120, and 150. Using random sampling, we are able to search for good hyperparameter values without having a preconceived notion of what is good. This, combined with an early stopping method, allows us to search many parameter values for good values without wasting too much computing power.**

**I also used a bandit policy that terminates when any runs best metric is less than 1/(1+0.1) or 91% of the best run's best metric.**

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
